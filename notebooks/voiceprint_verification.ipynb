{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "63bcb007",
      "metadata": {
        "id": "63bcb007"
      },
      "source": [
        "## Goal of the Voiceprint Model:\n",
        "To verify that a person speaking a specific phrase is the same, authorized user who just passed the facial recognition check. It's a binary classification task: \"Accept\" (voice matches the user) or \"Reject\" (voice does not match / unauthorized user)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11295eb9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "11295eb9"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import all required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.makedirs('/data/audio/', exist_ok=True)\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f66f7414",
      "metadata": {
        "id": "f66f7414"
      },
      "source": [
        "** Voiceprint Verification Model Development\n",
        "Phase 1: Data Collection & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd7cf90",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "5cd7cf90"
      },
      "outputs": [],
      "source": [
        "# Defining audio directory and check'\n",
        "\n",
        "def check_audio_files():\n",
        "    \"\"\"Check what audio files are available in the directory\"\"\"\n",
        "    if os.path.exists(AUDIO_DIR):\n",
        "        audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith(('.wav', '.mp3', '.m4a'))]\n",
        "        print(f\"Found {len(audio_files)} audio files:\")\n",
        "        for file in audio_files:\n",
        "            print(f\"  - {file}\")\n",
        "        return audio_files\n",
        "    else:\n",
        "        print(f\"Directory {AUDIO_DIR} does not exist. Please create it and add your audio files.\")\n",
        "        return []\n",
        "\n",
        "audio_files = check_audio_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e2e50e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "32e2e50e"
      },
      "outputs": [],
      "source": [
        "# Create sample audio data if no files exist (for demonstration)\n",
        "def create_sample_audio_files():\n",
        "    \"\"\"Create sample audio files for demonstration if none exist\"\"\"\n",
        "    sample_phrases = ['yes_approve', 'confirm_transaction']\n",
        "    users = ['user1', 'user2', 'user3']\n",
        "\n",
        "    # Create some sample audio files (sine waves simulating speech)\n",
        "    sample_rate = 22050\n",
        "    duration = 2.0\n",
        "\n",
        "    for user in users:\n",
        "        for phrase in sample_phrases:\n",
        "            filename = f\"{user}_{phrase}.wav\"\n",
        "            filepath = os.path.join(AUDIO_DIR, filename)\n",
        "\n",
        "            # Create a unique frequency pattern for each user+phrase combination\n",
        "            base_freq = 200 + (hash(user) % 100) + (hash(phrase) % 50)\n",
        "            t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "\n",
        "            # Generate a more complex waveform simulating speech\n",
        "            audio = 0.5 * np.sin(2 * np.pi * base_freq * t)\n",
        "            audio += 0.3 * np.sin(2 * np.pi * (base_freq * 2) * t)\n",
        "            audio += 0.2 * np.sin(2 * np.pi * (base_freq * 0.5) * t)\n",
        "\n",
        "            # Add some noise\n",
        "            audio += 0.05 * np.random.normal(0, 1, len(t))\n",
        "\n",
        "            # Normalize\n",
        "            audio = audio / np.max(np.abs(audio))\n",
        "\n",
        "            sf.write(filepath, audio, sample_rate)\n",
        "            print(f\"Created sample file: {filename}\")\n",
        "\n",
        "    return check_audio_files()\n",
        "\n",
        "if not audio_files:\n",
        "    print(\"Creating sample audio files for demonstration...\")\n",
        "    audio_files = create_sample_audio_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473367a3",
      "metadata": {
        "id": "473367a3"
      },
      "source": [
        "** Data Preprocessing & Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c13ef1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "82c13ef1"
      },
      "outputs": [],
      "source": [
        "# Load and visualize original audio samples\n",
        "def load_and_visualize_audio(filepath, title):\n",
        "    \"\"\"Load an audio file and create visualization\"\"\"\n",
        "    audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    fig.suptitle(f'Audio Analysis: {title}', fontsize=16)\n",
        "\n",
        "    # Waveform\n",
        "    librosa.display.waveshow(audio, sr=sr, ax=ax1, color='blue')\n",
        "    ax1.set_title('Waveform')\n",
        "    ax1.set_xlabel('Time (s)')\n",
        "    ax1.set_ylabel('Amplitude')\n",
        "\n",
        "    # Spectrogram\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
        "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax2)\n",
        "    ax2.set_title('Spectrogram')\n",
        "    plt.colorbar(img, ax=ax2, format='%+2.0f dB')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return audio, sr\n",
        "\n",
        "# Visualize first few files\n",
        "print(\"Visualizing sample audio files...\")\n",
        "for i, file in enumerate(audio_files[:2]):  # Show first 2 files\n",
        "    filepath = os.path.join(AUDIO_DIR, file)\n",
        "    audio, sr = load_and_visualize_audio(filepath, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b70d58",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "e2b70d58"
      },
      "outputs": [],
      "source": [
        "# Audio Augmentation Functions\n",
        "def augment_audio(audio, sr, augment_type):\n",
        "    \"\"\"Apply different types of audio augmentation\"\"\"\n",
        "    if augment_type == 'pitch_shift':\n",
        "        # Shift pitch by random semitones\n",
        "        n_steps = np.random.choice([-2, -1, 1, 2])\n",
        "        return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
        "\n",
        "    elif augment_type == 'time_stretch':\n",
        "        # Stretch or compress time\n",
        "        rate = np.random.choice([0.8, 0.9, 1.1, 1.2])\n",
        "        return librosa.effects.time_stretch(audio, rate=rate)\n",
        "\n",
        "    elif augment_type == 'add_noise':\n",
        "        # Add Gaussian noise\n",
        "        noise = np.random.normal(0, 0.005, audio.shape)\n",
        "        return audio + noise\n",
        "\n",
        "    elif augment_type == 'time_shift':\n",
        "        # Shift audio in time\n",
        "        shift = sr // 10  # Shift by 100ms\n",
        "        if np.random.random() > 0.5:\n",
        "            shift = -shift\n",
        "        return np.roll(audio, shift)\n",
        "\n",
        "    else:\n",
        "        return audio\n",
        "\n",
        "def apply_augmentations(audio, sr, filename):\n",
        "    \"\"\"Apply multiple augmentations and save results\"\"\"\n",
        "    augmentations = ['pitch_shift', 'time_stretch', 'add_noise', 'time_shift']\n",
        "    augmented_audios = []\n",
        "\n",
        "    print(f\"Applying augmentations for {filename}:\")\n",
        "\n",
        "    for i, aug_type in enumerate(augmentations[:2]):  # Apply 2 augmentations per file\n",
        "        augmented_audio = augment_audio(audio, sr, aug_type)\n",
        "        augmented_audios.append(augmented_audio)\n",
        "\n",
        "        # Save augmented file\n",
        "        aug_filename = f\"{os.path.splitext(filename)[0]}_{aug_type}.wav\"\n",
        "        aug_filepath = os.path.join(AUDIO_DIR, aug_filename)\n",
        "        sf.write(aug_filepath, augmented_audio, sr)\n",
        "        print(f\"  - Created: {aug_filename}\")\n",
        "\n",
        "        # Visualize one augmented sample\n",
        "        if i == 0:\n",
        "            load_and_visualize_audio(aug_filepath, f\"Augmented ({aug_type})\")\n",
        "\n",
        "    return augmented_audios\n",
        "\n",
        "# Apply augmentations to all original files\n",
        "print(\"\\nApplying audio augmentations...\")\n",
        "all_audio_data = {}\n",
        "for file in audio_files:\n",
        "    if 'augmented' not in file:  # Only augment original files\n",
        "        filepath = os.path.join(AUDIO_DIR, file)\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "        all_audio_data[file] = {'audio': audio, 'sr': sr}\n",
        "        apply_augmentations(audio, sr, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b946001",
      "metadata": {
        "id": "8b946001"
      },
      "source": [
        "** Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13591b9a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "13591b9a"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction Functions\n",
        "def extract_audio_features(audio, sr):\n",
        "    \"\"\"Extract comprehensive audio features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # MFCCs (Most important for voice recognition)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mfccs_mean = np.mean(mfccs, axis=1)\n",
        "    mfccs_std = np.std(mfccs, axis=1)\n",
        "\n",
        "    for i in range(13):\n",
        "        features[f'mfcc_mean_{i}'] = mfccs_mean[i]\n",
        "        features[f'mfcc_std_{i}'] = mfccs_std[i]\n",
        "\n",
        "    # Spectral features\n",
        "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
        "    features['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
        "    features['spectral_centroid_std'] = np.std(spectral_centroids)\n",
        "\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
        "    features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
        "    features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
        "\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
        "    features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
        "    features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
        "\n",
        "    # Zero-crossing rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio)\n",
        "    features['zcr_mean'] = np.mean(zcr)\n",
        "    features['zcr_std'] = np.std(zcr)\n",
        "\n",
        "    # RMS energy\n",
        "    rms = librosa.feature.rms(y=audio)\n",
        "    features['rms_mean'] = np.mean(rms)\n",
        "    features['rms_std'] = np.std(rms)\n",
        "\n",
        "    # Chroma features\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "    chroma_mean = np.mean(chroma, axis=1)\n",
        "    for i in range(12):\n",
        "        features[f'chroma_mean_{i}'] = chroma_mean[i]\n",
        "\n",
        "    # Additional temporal features\n",
        "    features['duration'] = len(audio) / sr\n",
        "    features['max_amplitude'] = np.max(np.abs(audio))\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_audio_features_dataset(audio_dir):\n",
        "    \"\"\"Create a comprehensive dataset from all audio files\"\"\"\n",
        "    features_list = []\n",
        "\n",
        "    audio_files = [f for f in os.listdir(audio_dir) if f.endswith(('.wav', '.mp3', '.m4a'))]\n",
        "\n",
        "    for filename in audio_files:\n",
        "        filepath = os.path.join(audio_dir, filename)\n",
        "\n",
        "        # Extract user_id and phrase from filename\n",
        "        parts = filename.replace('.wav', '').split('_')\n",
        "        user_id = parts[0]\n",
        "        phrase = '_'.join(parts[1:]) if len(parts) > 2 else parts[1]\n",
        "\n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "        # Extract features\n",
        "        features = extract_audio_features(audio, sr)\n",
        "\n",
        "        # Add metadata\n",
        "        features['filename'] = filename\n",
        "        features['user_id'] = user_id\n",
        "        features['phrase'] = phrase\n",
        "        features['is_original'] = 'augmented' not in filename\n",
        "\n",
        "        features_list.append(features)\n",
        "\n",
        "    return pd.DataFrame(features_list)\n",
        "\n",
        "print(\"Extracting audio features...\")\n",
        "audio_features_df = create_audio_features_dataset(AUDIO_DIR)\n",
        "print(f\"Extracted features from {len(audio_features_df)} audio files\")\n",
        "print(f\"Dataset shape: {audio_features_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9640992f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9640992f"
      },
      "outputs": [],
      "source": [
        "# Explore and Save Features\n",
        "# Display basic info about the features\n",
        "print(\"Audio Features Dataset Info:\")\n",
        "print(audio_features_df.info())\n",
        "\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(audio_features_df[['filename', 'user_id', 'phrase', 'is_original']].head())\n",
        "\n",
        "print(\"\\nUser distribution:\")\n",
        "print(audio_features_df['user_id'].value_counts())\n",
        "\n",
        "print(\"\\nPhrase distribution:\")\n",
        "print(audio_features_df['phrase'].value_counts())\n",
        "\n",
        "# Save features to CSV\n",
        "features_csv_path = 'audio_features.csv'\n",
        "audio_features_df.to_csv(features_csv_path, index=False)\n",
        "print(f\"\\nFeatures saved to: {features_csv_path}\")\n",
        "\n",
        "# Display feature statistics\n",
        "feature_columns = [col for col in audio_features_df.columns if col not in ['filename', 'user_id', 'phrase', 'is_original']]\n",
        "print(f\"\\nNumber of audio features: {len(feature_columns)}\")\n",
        "print(\"Feature statistics:\")\n",
        "print(audio_features_df[feature_columns[:5]].describe())  # Show first 5 features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a4275a",
      "metadata": {
        "id": "b5a4275a"
      },
      "source": [
        "** Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058d0d2b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "058d0d2b"
      },
      "outputs": [],
      "source": [
        "# Prepare Data for Model Training\n",
        "def prepare_voice_verification_data(df, target_user):\n",
        "    \"\"\"\n",
        "    Prepare data for voice verification\n",
        "    For a target user, create binary classification:\n",
        "    - Positive: All samples from target_user\n",
        "    - Negative: Samples from other users\n",
        "    \"\"\"\n",
        "    # Create labels\n",
        "    df['label'] = (df['user_id'] == target_user).astype(int)\n",
        "\n",
        "    # Select features (exclude metadata)\n",
        "    feature_cols = [col for col in df.columns if col not in\n",
        "                   ['filename', 'user_id', 'phrase', 'is_original', 'label']]\n",
        "\n",
        "    X = df[feature_cols]\n",
        "    y = df['label']\n",
        "\n",
        "    return X, y, feature_cols\n",
        "\n",
        "def train_voice_verification_model(X, y, test_size=0.3, random_state=42):\n",
        "    \"\"\"Train and evaluate a voice verification model\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train Random Forest model\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    return model, scaler, X_test_scaled, y_test, y_pred, y_pred_proba\n",
        "\n",
        "# Train model for each user\n",
        "print(\"Training voice verification models for each user...\")\n",
        "users = audio_features_df['user_id'].unique()\n",
        "models = {}\n",
        "scalers = {}\n",
        "results = {}\n",
        "\n",
        "for user in users:\n",
        "    print(f\"\\n--- Training model for {user} ---\")\n",
        "\n",
        "    # Prepare data\n",
        "    X, y, feature_cols = prepare_voice_verification_data(audio_features_df, user)\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, X_test, y_test, y_pred, y_pred_proba = train_voice_verification_model(X, y)\n",
        "\n",
        "    # Store models and scalers\n",
        "    models[user] = model\n",
        "    scalers[user] = scaler\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[user] = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'feature_columns': feature_cols\n",
        "    }\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"Test set size: {len(y_test)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73cd27ee",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "73cd27ee"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation and Visualization\n",
        "def evaluate_model_performance(results):\n",
        "    \"\"\"Evaluate and display model performance\"\"\"\n",
        "    print(\"=== VOICEPRINT VERIFICATION MODEL RESULTS ===\\n\")\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "\n",
        "    # Display results\n",
        "    for user, metrics in results.items():\n",
        "        print(f\"User: {user}\")\n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
        "        print(f\"  Number of Features: {len(metrics['feature_columns'])}\")\n",
        "        print()\n",
        "\n",
        "    # Plot performance comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    x = range(len(results_df))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar([i - width/2 for i in x], results_df['accuracy'], width, label='Accuracy', alpha=0.7)\n",
        "    plt.bar([i + width/2 for i in x], results_df['f1_score'], width, label='F1-Score', alpha=0.7)\n",
        "\n",
        "    plt.xlabel('Users')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Voice Verification Model Performance by User')\n",
        "    plt.xticks(x, results_df.index)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(0, 1.1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, (acc, f1) in enumerate(zip(results_df['accuracy'], results_df['f1_score'])):\n",
        "        plt.text(i - width/2, acc + 0.02, f'{acc:.3f}', ha='center', va='bottom')\n",
        "        plt.text(i + width/2, f1 + 0.02, f'{f1:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Evaluate all models\n",
        "results_df = evaluate_model_performance(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf85558",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "abf85558"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Analysis\n",
        "def plot_feature_importance(models, feature_cols, top_n=15):\n",
        "    \"\"\"Plot the most important features across all models\"\"\"\n",
        "    # Calculate average feature importance across all models\n",
        "    avg_importance = np.zeros(len(feature_cols))\n",
        "\n",
        "    for user, model in models.items():\n",
        "        avg_importance += model.feature_importances_\n",
        "\n",
        "    avg_importance /= len(models)\n",
        "\n",
        "    # Create feature importance dataframe\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': avg_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Plot top N features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_importance_df.head(top_n)\n",
        "\n",
        "    plt.barh(range(len(top_features)), top_features['importance'][::-1])\n",
        "    plt.yticks(range(len(top_features)), top_features['feature'][::-1])\n",
        "    plt.xlabel('Average Feature Importance')\n",
        "    plt.title(f'Top {top_n} Most Important Audio Features Across All Models')\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return feature_importance_df\n",
        "\n",
        "# Plot feature importance\n",
        "feature_importance_df = plot_feature_importance(models, feature_cols)\n",
        "print(\"Top 10 most important features:\")\n",
        "print(feature_importance_df.head(10)[['feature', 'importance']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5cae8fb",
      "metadata": {
        "id": "f5cae8fb"
      },
      "source": [
        "** Integration & System Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55e7568",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d55e7568"
      },
      "outputs": [],
      "source": [
        "# Voice Verification Function\n",
        "class VoiceVerificationSystem:\n",
        "    def __init__(self, models, scalers, feature_columns):\n",
        "        self.models = models\n",
        "        self.scalers = scalers\n",
        "        self.feature_columns = feature_columns\n",
        "\n",
        "    def verify_voice(self, audio_filepath, claimed_user_id):\n",
        "        \"\"\"\n",
        "        Verify if the voice in audio_file matches the claimed_user_id\n",
        "        Returns: (is_verified, confidence_score)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load and extract features from the new audio\n",
        "            audio, sr = librosa.load(audio_filepath, sr=None)\n",
        "            features = extract_audio_features(audio, sr)\n",
        "\n",
        "            # Create feature vector in correct order\n",
        "            feature_vector = []\n",
        "            for col in self.feature_columns:\n",
        "                if col in features:\n",
        "                    feature_vector.append(features[col])\n",
        "                else:\n",
        "                    feature_vector.append(0)  # Default value if feature missing\n",
        "\n",
        "            feature_vector = np.array(feature_vector).reshape(1, -1)\n",
        "\n",
        "            # Scale features\n",
        "            if claimed_user_id in self.scalers:\n",
        "                feature_vector_scaled = self.scalers[claimed_user_id].transform(feature_vector)\n",
        "            else:\n",
        "                return False, 0.0\n",
        "\n",
        "            # Predict\n",
        "            if claimed_user_id in self.models:\n",
        "                model = self.models[claimed_user_id]\n",
        "                prediction = model.predict(feature_vector_scaled)[0]\n",
        "                confidence = model.predict_proba(feature_vector_scaled)[0, 1]\n",
        "\n",
        "                return bool(prediction), confidence\n",
        "            else:\n",
        "                return False, 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in voice verification: {e}\")\n",
        "            return False, 0.0\n",
        "\n",
        "# Initialize the verification system\n",
        "voice_system = VoiceVerificationSystem(models, scalers, feature_cols)\n",
        "print(\"Voice Verification System initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f78bac",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "51f78bac"
      },
      "outputs": [],
      "source": [
        "# System Demonstration\n",
        "def simulate_authentication_flow():\n",
        "    \"\"\"Simulate the complete authentication flow\"\"\"\n",
        "    print(\"=== AUTHENTICATION SYSTEM DEMONSTRATION ===\\n\")\n",
        "\n",
        "    # Test cases\n",
        "    test_cases = [\n",
        "        # (audio_file, claimed_user, expected_result, description)\n",
        "        ('user1_yes.wav', 'user1', True, \"Valid user with correct phrase\"),\n",
        "        ('user2_confirm.wav', 'user2', True, \"Valid user with correct phrase\"),\n",
        "        ('user1_yes.wav', 'user2', False, \"Wrong user claim\"),\n",
        "        ('user2_confirm.wav', 'user1', False, \"Wrong user claim\"),\n",
        "    ]\n",
        "\n",
        "    print(\"Testing authentication flow:\\n\")\n",
        "\n",
        "    for audio_file, claimed_user, expected, description in test_cases:\n",
        "        audio_path = os.path.join(AUDIO_DIR, audio_file)\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            # Simulate facial recognition (would normally come from face model)\n",
        "            print(f\"üîπ Step 1: Facial Recognition\")\n",
        "            print(f\"   - User presents face ‚Üí Recognized as '{claimed_user}'\")\n",
        "\n",
        "            # Voice verification\n",
        "            print(f\"üîπ Step 2: Voice Verification\")\n",
        "            print(f\"   - User says phrase from: {audio_file}\")\n",
        "\n",
        "            is_verified, confidence = voice_system.verify_voice(audio_path, claimed_user)\n",
        "\n",
        "            print(f\"   - Voice Match: {'‚úÖ ACCEPT' if is_verified else '‚ùå REJECT'}\")\n",
        "            print(f\"   - Confidence: {confidence:.4f}\")\n",
        "            print(f\"   - Expected: {'‚úÖ' if is_verified == expected else '‚ùå'}\")\n",
        "\n",
        "            # Product recommendation (if both steps pass)\n",
        "            if is_verified:\n",
        "                print(f\"üîπ Step 3: Product Recommendation\")\n",
        "                print(f\"   - ‚úÖ ACCESS GRANTED\")\n",
        "                print(f\"   - Displaying personalized products for {claimed_user}...\")\n",
        "                # Here you would integrate with the product recommendation model\n",
        "                products = [\"Smart Watch Pro\", \"Wireless Earbuds\", \"Fitness Tracker\"]\n",
        "                print(f\"   - Recommended: {np.random.choice(products)}\")\n",
        "            else:\n",
        "                print(f\"üîπ Step 3: Access Denied\")\n",
        "                print(f\"   - ‚ùå ACCESS DENIED - Voice verification failed\")\n",
        "\n",
        "            print(f\"   Description: {description}\")\n",
        "            print(\"-\" * 60)\n",
        "        else:\n",
        "            print(f\"‚ùå Test file not found: {audio_file}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "# Run demonstration\n",
        "simulate_authentication_flow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb2242e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1bb2242e"
      },
      "outputs": [],
      "source": [
        "# Simulate Unauthorized Attempt\n",
        "def simulate_unauthorized_attempt():\n",
        "    \"\"\"Simulate an unauthorized access attempt\"\"\"\n",
        "    print(\"\\n=== SIMULATING UNAUTHORIZED ACCESS ATTEMPT ===\\n\")\n",
        "\n",
        "    # Simulate someone trying to impersonate a user\n",
        "    print(\"üö® SECURITY SCENARIO: Impersonation Attempt\")\n",
        "    print(\"An unauthorized person tries to access user1's account...\\n\")\n",
        "\n",
        "    # They somehow pass facial recognition (stolen photo?)\n",
        "    print(\"üîπ Step 1: Facial Recognition (bypassed with photo)\")\n",
        "    print(\"   - System recognizes: 'user1'\")\n",
        "\n",
        "    # But their voice doesn't match\n",
        "    print(\"üîπ Step 2: Voice Verification\")\n",
        "\n",
        "    # Try different unauthorized scenarios\n",
        "    unauthorized_scenarios = [\n",
        "        ('user2_yes.wav', 'user2 trying to impersonate user1'),\n",
        "        ('user3_confirm.wav', 'user3 trying to impersonate user1'),\n",
        "    ]\n",
        "\n",
        "    for audio_file, scenario in unauthorized_scenarios:\n",
        "        audio_path = os.path.join(AUDIO_DIR, audio_file)\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            is_verified, confidence = voice_system.verify_voice(audio_path, 'user1')\n",
        "\n",
        "            print(f\"   - Scenario: {scenario}\")\n",
        "            print(f\"   - Voice Match: {'‚ùå REJECT' if not is_verified else '‚ö†Ô∏è  FALSE ACCEPT (Security Breach!)'}\")\n",
        "            print(f\"   - Confidence: {confidence:.4f}\")\n",
        "\n",
        "            if not is_verified:\n",
        "                print(\"   - ‚úÖ SECURITY SYSTEM WORKING: Unauthorized access prevented!\")\n",
        "            else:\n",
        "                print(\"   - üö® SECURITY BREACH: System incorrectly accepted unauthorized user!\")\n",
        "\n",
        "            print(\"   \" + \"-\" * 40)\n",
        "        else:\n",
        "            print(f\"   - Test file not found: {audio_file}\")\n",
        "\n",
        "# Run unauthorized attempt simulation\n",
        "simulate_unauthorized_attempt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "714741e5",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "714741e5"
      },
      "outputs": [],
      "source": [
        "# Save the Complete Model System\n",
        "import joblib\n",
        "\n",
        "def save_complete_system():\n",
        "    \"\"\"Save all models and system components\"\"\"\n",
        "    system_components = {\n",
        "        'voice_models': models,\n",
        "        'scalers': scalers,\n",
        "        'feature_columns': feature_cols,\n",
        "        'voice_system': voice_system,\n",
        "        'audio_features_df': audio_features_df,\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    joblib.dump(system_components, 'voice_verification_system.pkl')\n",
        "    print(\"‚úÖ Complete voice verification system saved to 'voice_verification_system.pkl'\")\n",
        "\n",
        "    # Also save feature importance\n",
        "    feature_importance_df.to_csv('feature_importance.csv', index=False)\n",
        "    print(\"‚úÖ Feature importance saved to 'feature_importance.csv'\")\n",
        "\n",
        "save_complete_system()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VOICEPRINT VERIFICATION SYSTEM DEVELOPMENT COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"‚Ä¢ Users in system: {list(users)}\")\n",
        "print(f\"‚Ä¢ Total audio samples processed: {len(audio_features_df)}\")\n",
        "print(f\"‚Ä¢ Number of audio features extracted: {len(feature_cols)}\")\n",
        "print(f\"‚Ä¢ Average accuracy across users: {results_df['accuracy'].mean():.4f}\")\n",
        "print(f\"‚Ä¢ Average F1-score across users: {results_df['f1_score'].mean():.4f}\")\n",
        "print(f\"‚Ä¢ Models saved and ready for integration with facial recognition system\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}